{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmentation\n",
    "\n",
    "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
    "\n",
    "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
    "\n",
    "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\VisualCode\\\\chatbot_ai\\\\chatbot_ai\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\VisualCode\\\\chatbot_ai\\\\chatbot_ai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDFs using langchain\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = load_pdf_file(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks \n",
    "def text_split(extracted_text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
    "                                                   chunk_overlap=128)\n",
    "    text_chunks = text_splitter.split_documents(extracted_text)\n",
    "    return text_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks:  18761\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_text)\n",
    "print(\"Length of Text Chunks: \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'creationdate': '2017-03-01T18:01:20+00:00', 'author': 'Martin Kleppmann', 'moddate': '2017-03-01T13:07:54-05:00', 'title': 'Designing Data-Intensive Applications', 'trapped': '/False', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'total_pages': 613, 'page': 0, 'page_label': 'Cover'}, page_content='Martin Kleppmann\\nDesigning \\nData-Intensive \\nApplications\\nTHE BIG IDEAS BEHIND RELIABLE, SCALABLE,  \\nAND MAINTAINABLE SYSTEMS')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding model and querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embeddings from Hugging Face\n",
    "embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "def download_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embed_model, model_kwargs=model_kwargs)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query length:  384\n"
     ]
    }
   ],
   "source": [
    "query = \"This is an example sentence\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "print(\"Query length: \", len(query_embedding)) # dimension of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06765691936016083,\n",
       " 0.0634959414601326,\n",
       " 0.0487130731344223,\n",
       " 0.07930496335029602,\n",
       " 0.037448056042194366,\n",
       " 0.0026527722366154194,\n",
       " 0.039374940097332,\n",
       " -0.007098493631929159,\n",
       " 0.05936146154999733,\n",
       " 0.03153702989220619]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Index and Initializing Connection to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'asura'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  # dimensionality of all-MiniLM-L6-v2\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 20529}},\n",
       " 'total_vector_count': 20529}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "index = pc.Index(index_name)\n",
    "# wait a moment for connection\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Initializing VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "vectorestore_from_docs = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing Index\n",
    "\n",
    "vectorestore_from_docs = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "retriever = vectorestore_from_docs.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\":3}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs1 = retriever.invoke(\"What is Data Flow\")\n",
    "retrieved_docs2 = retriever.invoke(\"What is Neural Network\")\n",
    "retrieved_docs3 = retriever.invoke(\"Who is Data Engineer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='acf8490e-3b47-4751-9bf7-138a127690f4', metadata={'author': 'Martin Kleppmann', 'creationdate': '2017-03-01T18:01:20+00:00', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'moddate': '2017-03-01T13:07:54-05:00', 'page': 150.0, 'page_label': '129', 'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'title': 'Designing Data-Intensive Applications', 'total_pages': 613.0, 'trapped': '/False'}, page_content='That’s a fairly abstract idea—there are many ways data can flow from one process to\\nanother. Who encodes the data, and who decodes it? In the rest of this chapter we\\nwill explore some of the most common ways how data flows between processes:\\n• Via databases (see “Dataflow Through Databases” on page 129)\\n• Via service calls (see “Dataflow Through Services: REST and RPC” on page 131)\\n• Via asynchronous message passing (see “Message-Passing Dataflow” on page 136)\\nDataflow Through Databases'),\n",
       " Document(id='f3939261-84f1-4265-9668-cb618369c16d', metadata={'author': 'Martin Kleppmann', 'creationdate': '2017-03-01T18:01:20+00:00', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'moddate': '2017-03-01T13:07:54-05:00', 'page': 565.0, 'page_label': '544', 'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'title': 'Designing Data-Intensive Applications', 'total_pages': 613.0, 'trapped': '/False'}, page_content='code on the whole input dataset in order to rederive the output. Similarly, if some‐\\nthing goes wrong, you can fix the code and reprocess the data in order to recover.\\nThese processes are quite similar to what databases already do internally, so we recast\\nthe idea of dataflow applications as unbundling the components of a database, and\\nbuilding an application by composing these loosely coupled components.\\nDerived state can be updated by observing changes in the underlying data. Moreover,'),\n",
       " Document(id='6743811b-a717-4305-a14d-a6c0db39f5fd', metadata={'author': 'Martin Kleppmann', 'creationdate': '2017-03-01T18:01:20+00:00', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'moddate': '2017-03-01T13:07:54-05:00', 'page': 149.0, 'page_label': '128', 'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'title': 'Designing Data-Intensive Applications', 'total_pages': 613.0, 'trapped': '/False'}, page_content='model” on page 39), while also providing better guarantees about your data and bet‐\\nter tooling. \\nModes of Dataflow\\nAt the beginning of this chapter we said that whenever you want to send some data to\\nanother process with which you don’t share memory—for example, whenever you\\nwant to send data over the network or write it to a file—you need to encode it as a\\nsequence of bytes. We then discussed a variety of different encodings for doing this.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ca0e5a25-291b-4c9b-a997-2e02af8388cb', metadata={'author': 'Rabunal, Juan Ramon; Dorado, Julian; Pazos Sierra, Alejandro.', 'creationdate': '2008-07-02T14:52:44-04:00', 'creator': 'Adobe InDesign CS2 (4.0.5)', 'moddate': '2008-09-23T21:53:54+02:00', 'page': 70.0, 'page_label': '35', 'producer': 'Adobe PDF Library 7.0', 'source': 'data\\\\encyclopedia-of-artificial-intelligence.pdf', 'title': 'Encyclopedia of Artificial Intelligence', 'total_pages': 1677.0}, page_content='and Applications ISDA ‘05, 141 – 146. Institute of \\nElectrical & Electronics Engineering Publisher.\\nKEy TERMS\\nArtificial Neural Networks (ANN): An artificial \\nneural network, often just called a “neural network” \\n(NN), is an interconnected group of artificial neurons \\nthat uses a mathematical model or computational model \\nfor information processing based on a connectionist \\napproach to computation. Knowledge is acquired by \\nthe network from its environment through a learning'),\n",
       " Document(id='c702a7ac-6fa0-447f-9afd-166ec0c3fdd8', metadata={'author': 'Rabunal, Juan Ramon; Dorado, Julian; Pazos Sierra, Alejandro.', 'creationdate': '2008-07-02T14:52:44-04:00', 'creator': 'Adobe InDesign CS2 (4.0.5)', 'moddate': '2008-09-23T21:53:54+02:00', 'page': 269.0, 'page_label': '234', 'producer': 'Adobe PDF Library 7.0', 'source': 'data\\\\encyclopedia-of-artificial-intelligence.pdf', 'title': 'Encyclopedia of Artificial Intelligence', 'total_pages': 1677.0}, page_content='Neural Networks applied to Document Enhancement. \\nIn: Computational and Ambient Intelligence. V olume \\n4507 of LNCS. Springer-Verlag. 144-151.\\nhttp://en.wikipedia.org\\nKEy TERMS\\nArtificial Neural Network: An artificial neural \\nnetwork (ANN), often just called a “neural network” \\n(NN), is an interconnected group of artificial neurons \\nthat uses a mathematical model or computational model \\nfor information processing based on a connectionist \\napproach to computation.\\nBackpropagation Algorith m: A supervised'),\n",
       " Document(id='170d5bad-1e3b-48c7-900b-b3d9dd86f2e4', metadata={'author': 'Rabunal, Juan Ramon; Dorado, Julian; Pazos Sierra, Alejandro.', 'creationdate': '2008-07-02T14:52:44-04:00', 'creator': 'Adobe InDesign CS2 (4.0.5)', 'moddate': '2008-09-23T21:53:54+02:00', 'page': 1370.0, 'page_label': '1335', 'producer': 'Adobe PDF Library 7.0', 'source': 'data\\\\encyclopedia-of-artificial-intelligence.pdf', 'title': 'Encyclopedia of Artificial Intelligence', 'total_pages': 1677.0}, page_content='in natural system necessary for evolution, specifically \\nthose that follow the principles first laid down by Charles \\nDarwin of survival of the fittest. As such they represent \\nan intelligent exploitation of a random search within a \\ndefined search space to solve a problem.\\nNeural Network: A Neural Network is an informa-\\ntion processing paradigm that is inspired by the way \\nbiological nervous systems, such as the brain, process \\ninformation. The key element of this paradigm is the')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='013b1f02-80f2-409c-8353-106b350048e0', metadata={'author': 'Martin Kleppmann', 'creationdate': '2017-03-01T18:01:20+00:00', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'moddate': '2017-03-01T13:07:54-05:00', 'page': 565.0, 'page_label': '544', 'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'title': 'Designing Data-Intensive Applications', 'total_pages': 613.0, 'trapped': '/False'}, page_content='data breaches, and we may find that a well-intentioned use of data has unintended\\nconsequences.\\nAs software and data are having such a large impact on the world, we engineers must\\nremember that we carry a responsibility to work toward the kind of world that we\\nwant to live in: a world that treats people with humanity and respect. I hope that we\\ncan work together toward that goal. \\n544 | Chapter 12: The Future of Data Systems'),\n",
       " Document(id='a7a53198-b401-4e23-a300-35d0a231015d', metadata={'author': 'Martin Kleppmann', 'creationdate': '2017-03-01T18:01:20+00:00', 'creator': 'AH CSS Formatter V6.0 MR2 for Linux64 : 6.0.2.5372 (2012/05/16 18:26JST)', 'moddate': '2017-03-01T13:07:54-05:00', 'page': 554.0, 'page_label': '533', 'producer': 'Antenna House PDF Output Library 2.6.0 (Linux64)', 'source': 'data\\\\Designing_Data-Intensive_Applications_TH.pdf', 'title': 'Designing Data-Intensive Applications', 'total_pages': 613.0, 'trapped': '/False'}, page_content='Every system is built for a purpose; every action we take has both intended and unin‐\\ntended consequences. The purpose may be as simple as making money, but the con‐\\nsequences for the world may reach far beyond that original purpose. We, the\\nengineers building these systems, have a responsibility to carefully consider those\\nconsequences and to consciously decide what kind of world we want to live in.\\nWe talk about data as an abstract thing, but remember that many datasets are about'),\n",
       " Document(id='5e39f997-1787-4794-b194-fa5993e94ad6', metadata={'author': 'Rabunal, Juan Ramon; Dorado, Julian; Pazos Sierra, Alejandro.', 'creationdate': '2008-07-02T14:52:44-04:00', 'creator': 'Adobe InDesign CS2 (4.0.5)', 'moddate': '2008-09-23T21:53:54+02:00', 'page': 19.0, 'page_label': 'xviii', 'producer': 'Adobe PDF Library 7.0', 'source': 'data\\\\encyclopedia-of-artificial-intelligence.pdf', 'title': 'Encyclopedia of Artificial Intelligence', 'total_pages': 1677.0}, page_content='Qiyang Chen, Montclair State University, USA; and James Yao, Montclair State University, USA ...............418\\nData Warehousing Development and Design Methodologies / James Yao, Montclair State University, \\nUSA; and John Wang, Montclair State University, USA .................................................................................424\\nDecision Making in Intelligent Agents / Mats Danielson, Stockholm University, Sweden &')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Question-Answering\n",
    "**Setup Groq Model**\n",
    "\n",
    "All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA.\n",
    "\n",
    "## Generative Question-Answering\n",
    "\n",
    "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
    "\n",
    "To do this we initialize a `RetrievalQA` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm_model = \"llama3-8b-8192\"\n",
    "max_tokens = 512\n",
    "temperature = 0.4\n",
    "llm = ChatGroq(temperature=temperature, model_name=llm_model, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert-level Data Engineering and Artificial Intelligence Consultant tasked with providing precise, comprehensive, and actionable insights on complex technical queries.\"\n",
    "    \"Your domain expertise encompasses advanced data architectures, cloud-based data solutions, ETL processes, big data frameworks, and cutting-edge machine learning and deep learning methodologies.\"\n",
    "    \"In every response, adhere strictly to corporate communication standards—employ a formal, respectful tone and use industry-specific terminology.\"\n",
    "    \"Your explanations must be methodically reasoned, grounded in current best practices, and aligned with the highest standards of technical accuracy and ethical responsibility.\" \n",
    "    \"Ensure that each recommendation is both innovative and pragmatic, supporting strategic decision-making in the realms of data engineering and AI.\"\n",
    "    \"/n/n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A data pipeline, also known as a dataflow or data stream, is a sequence of processes that extract, transform, and load (ETL) data from various sources, transform it into a desired format, and load it into a target system, such as a data warehouse, database, or data lake. The primary goal of a data pipeline is to efficiently and reliably move data from its source to its destination, enabling data-driven decision-making and analytics.\n",
      "\n",
      "A data pipeline typically consists of several components, including:\n",
      "\n",
      "1. Data Sources: These are the systems, applications, or devices that generate data, such as databases, files, APIs, or IoT devices.\n",
      "2. Data Processing: This involves transforming and manipulating the data to ensure it is in the desired format and meets the required quality standards.\n",
      "3. Data Storage: This is where the processed data is stored, such as a data warehouse, database, or data lake.\n",
      "4. Data Consumers: These are the applications, services, or users that consume the processed data, such as business intelligence tools, data visualization dashboards, or machine learning models.\n",
      "\n",
      "Data pipelines can be designed to handle various types of data, including:\n",
      "\n",
      "1. Batch processing: Processing large volumes of data in batches, typically used for reporting and analytics.\n",
      "2. Stream processing: Processing data in real-time, typically used for event-driven applications and IoT data.\n",
      "3. Real-time processing: Processing data as it is generated, typically used for applications that require immediate processing and analysis.\n",
      "\n",
      "Data pipelines offer several benefits, including:\n",
      "\n",
      "1. Improved data quality: By processing and transforming data in a controlled and standardized manner, data pipelines can ensure data consistency and accuracy.\n",
      "2. Increased efficiency: Data pipelines can automate repetitive tasks, reducing the need for manual intervention and minimizing the risk of human error.\n",
      "3. Scalability: Data pipelines can be designed to handle large volumes of data and scale horizontally to meet increasing demands.\n",
      "4. Flexibility: Data pipelines can be easily modified or extended to accommodate changing data requirements or new data sources.\n",
      "\n",
      "In summary, a data pipeline is a critical component of modern data management, enabling organizations to extract, transform, and load data efficiently and reliably, while ensuring data quality, scalability, and flexibility.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Data Pipeline?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help! However, I notice that the provided text doesn't mention Acne. Instead, it appears to be discussing biometric systems, face detection, and related concepts.\n",
      "\n",
      "If you meant to ask about acne, I'd be happy to provide information. Acne is a common skin condition characterized by the appearance of pimples, blackheads, and whiteheads on the skin. It occurs when the pores on the skin become clogged with dead skin cells, oil, and bacteria, leading to inflammation and infection.\n",
      "\n",
      "If you'd like to know more about acne or have any specific questions, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
